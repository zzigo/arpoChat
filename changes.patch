diff --git a/api/generate.py b/api/generate.py
index b15259d..93c810a 100644
--- a/api/generate.py
+++ b/api/generate.py
@@ -1,344 +1,131 @@
-from fastapi import FastAPI, HTTPException, Request
-from fastapi.middleware.cors import CORSMiddleware
-from fastapi.responses import JSONResponse
-from pydantic import BaseModel
 import os
-from typing import Optional, List, Dict, Any
-import logging
-import torch
 from transformers import GPT2LMHeadModel, GPT2Tokenizer
-from huggingface_hub import hf_hub_download
+import torch
+from typing import List, Dict, Any, Optional
+import logging
+from huggingface_hub import HfApi
+import gc
 
-# Configure logging
-logging.basicConfig(
-    level=logging.DEBUG,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
-)
+# Setup logging
+logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
-app = FastAPI(
-    title="ArpoChat API",
-    description="API for ArpoChat text generation",
-    version="1.0.0"
-)
-
-# Enable CORS with proper configuration
-app.add_middleware(
-    CORSMiddleware,
-    allow_origins=[
-        "http://localhost:8004",
-        "http://localhost:3000",
-        "http://127.0.0.1:8004",
-        "http://127.0.0.1:3000",
-        "https://*.onrender.com",  # Allow Render subdomains
-    ],
-    allow_credentials=True,
-    allow_methods=["*"],
-    allow_headers=["*"],
-    expose_headers=["*"],
-    max_age=3600,  # Cache preflight requests for 1 hour
-)
-
-# Constants
-LOCAL_MODEL_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "model", "model.pt")
-HF_MODEL_REPO = "animaratio/arpochat"  # Your Hugging Face model repository
-HF_MODEL_FILENAME = "model.pt"  # The filename in your HF repo
-DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+# Get HF token from environment
+hf_token = os.getenv('HF_TOKEN')
+if not hf_token:
+    raise EnvironmentError("HF_TOKEN environment variable is not set")
 
-# Global variables for model caching
-_model = None
-_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # Initialize tokenizer immediately
-_tokenizer.pad_token = _tokenizer.eos_token  # Configure tokenizer
+# Initialize Hugging Face API
+api = HfApi(token=hf_token)
 
-def load_model_weights():
-    """Load model weights either from local path or Hugging Face"""
-    # First try local path (for development)
-    if os.path.exists(LOCAL_MODEL_PATH):
-        logger.info(f"Loading model from local path: {LOCAL_MODEL_PATH}")
-        return torch.load(LOCAL_MODEL_PATH, map_location=DEVICE)
+# Initialize tokenizer and model with token
+try:
+    logger.info("Initializing tokenizer and model...")
+    # First verify token is valid
+    api.whoami()
     
-    # If local file not found, try Hugging Face (for production)
-    try:
-        logger.info(f"Local model not found, downloading from Hugging Face: {HF_MODEL_REPO}")
-        hf_token = os.getenv('HF_TOKEN')
-        if not hf_token:
-            raise ValueError("HF_TOKEN environment variable not set")
-            
-        model_path = hf_hub_download(
-            repo_id=HF_MODEL_REPO,
-            filename=HF_MODEL_FILENAME,
-            token=hf_token
-        )
-        return torch.load(model_path, map_location=DEVICE)
-    except Exception as e:
-        logger.error(f"Error downloading model from Hugging Face: {str(e)}")
-        raise
-
-def get_model():
-    """Load and cache the model and tokenizer."""
-    global _model
+    # Initialize with authentication
+    _tokenizer = GPT2Tokenizer.from_pretrained(
+        'gpt2',
+        use_auth_token=hf_token,
+        local_files_only=False  # Force download from HF
+    )
     
-    try:
-        if _model is None:
-            # Initialize model architecture
-            _model = GPT2LMHeadModel.from_pretrained('gpt2')
-            
-            # Load custom weights (either local or from HF)
-            state_dict = load_model_weights()
-            _model.load_state_dict(state_dict)
-            
-            # Configure model
-            _model.config.pad_token_id = _model.config.eos_token_id
-            _model.to(DEVICE)
-            _model.eval()
-            
-            logger.info("Model loaded successfully")
-        
-        return _model
-    except Exception as e:
-        logger.error(f"Error loading model: {str(e)}")
-        return None
-
-class GenerateRequest(BaseModel):
-    prompt: str
-    max_length: Optional[int] = 1000
-    temperatures: Optional[List[float]] = [0.7]
+    # Load model with memory optimizations
+    _model = GPT2LMHeadModel.from_pretrained(
+        'gpt2',
+        use_auth_token=hf_token,
+        local_files_only=False,  # Force download from HF
+        low_cpu_mem_usage=True,
+        torch_dtype=torch.float32  # Use float32 instead of float16 for better compatibility
+    )
+    
+    # Move model to appropriate device
+    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+    _model.to(device)
+    
+    # Enable evaluation mode
+    _model.eval()
+    
+    logger.info("Model and tokenizer initialized successfully")
+except Exception as e:
+    logger.error(f"Error initializing model or tokenizer: {str(e)}")
+    raise
 
-class GenerateResponse(BaseModel):
-    success: bool
-    prompt: str
-    responses: List[Dict[str, Any]]
-    stats: Optional[Dict[str, Any]] = None
+def get_model():
+    """Return the loaded model."""
+    return _model
 
 def generate_text(
-    model: GPT2LMHeadModel,
     prompt: str,
-    max_length: Optional[int] = 1000,
-    temperature: Optional[float] = 0.7
-) -> str:
-    """Generate text using the loaded model with enhanced semantic coherence."""
+    max_length: int = 100,
+    num_return_sequences: int = 3,
+    temperature: float = 0.8,
+    top_k: int = 50,
+    top_p: float = 0.95,
+    repetition_penalty: float = 1.2,
+    early_stopping: bool = True,
+) -> List[str]:
+    """
+    Generate multiple poetic responses for a given prompt.
+    
+    Args:
+        prompt: Input text to generate from
+        max_length: Maximum length of generated text
+        num_return_sequences: Number of different sequences to generate
+        temperature: Higher values = more creative, lower = more focused
+        top_k: Number of highest probability tokens to consider
+        top_p: Cumulative probability threshold for token selection
+        repetition_penalty: Penalty for repeating tokens
+        early_stopping: Whether to stop at the first complete sentence
+    
+    Returns:
+        List of generated text sequences
+    """
     try:
-        # Analyze prompt for key elements
-        prompt_tokens = _tokenizer.encode(prompt, add_special_tokens=False)
-        prompt_length = len(prompt_tokens)
+        # Clear CUDA cache if available
+        if torch.cuda.is_available():
+            torch.cuda.empty_cache()
         
-        # Encode the prompt with special handling
-        inputs = _tokenizer(prompt, return_tensors='pt', padding=True)
-        input_ids = inputs['input_ids'].to(DEVICE)
-        attention_mask = inputs['attention_mask'].to(DEVICE)
+        # Encode input
+        inputs = _tokenizer.encode(prompt, return_tensors="pt").to(device)
         
-        # Enhanced generation parameters for semantic bridging
+        # Set up generation parameters
         gen_kwargs = {
-            'input_ids': input_ids,
-            'max_length': max_length + prompt_length,  # Account for prompt length
-            'num_return_sequences': 1,
-            'do_sample': True,
-            'temperature': temperature,
-            'top_k': 40,  # More focused word choice for coherence
-            'top_p': 0.85,  # More selective sampling
-            'no_repeat_ngram_size': 3,  # Allow some repetition for style
-            'num_beams': 3,  # Reduced for more natural flow
-            'length_penalty': 1.3,  # Encourage development of ideas
-            'repetition_penalty': 1.15,  # Slightly increased to avoid exact prompt repetition
-            'pad_token_id': _tokenizer.eos_token_id,
-            'eos_token_id': _tokenizer.eos_token_id,
-            'early_stopping': True
-        }
-        
-        # Generate initial text
-        with torch.no_grad():
-            output_sequences = model.generate(**gen_kwargs)
-        
-        # Process the generated text
-        generated_text = _tokenizer.decode(output_sequences[0], skip_special_tokens=True)
-        
-        # Remove the prompt but maintain flow
-        if generated_text.startswith(prompt):
-            # Find a good breakpoint after the prompt
-            prompt_end = len(prompt)
-            next_sentence = generated_text.find('. ', prompt_end)
-            next_line = generated_text.find('\n', prompt_end)
-            
-            # Choose the closer breakpoint
-            if next_sentence != -1 and next_line != -1:
-                break_point = min(next_sentence, next_line)
-            else:
-                break_point = max(next_sentence, next_line)
-                
-            if break_point == -1:
-                break_point = prompt_end
-            
-            # Remove the prompt and any partial sentence/line
-            generated_text = generated_text[break_point + 1:].strip()
-        
-        # If the text is too short or lacks coherence
-        if len(generated_text.split()) < max_length // 8:
-            gen_kwargs.update({
-                'temperature': min(temperature + 0.1, 0.95),
-                'top_p': 0.9,
-                'repetition_penalty': 1.1
-            })
-            with torch.no_grad():
-                output_sequences = model.generate(**gen_kwargs)
-            generated_text = _tokenizer.decode(output_sequences[0], skip_special_tokens=True)
-            if generated_text.startswith(prompt):
-                generated_text = generated_text[len(prompt):].strip()
-        
-        return generated_text.strip()
-        
-    except Exception as e:
-        logger.error(f"Error generating text: {str(e)}")
-        raise
-
-def generate_fallback_response(prompt: str, max_length: int, temperature: float) -> dict:
-    """Generate a fallback response when the model is not available"""
-    response_text = f"Test response for prompt: {prompt}"
-    return {
-        'success': True,
-        'prompt': prompt,
-        'responses': [{
-            'temperature': temperature,
-            'text': response_text
-        }],
-        'stats': {
-            'total_chars': len(response_text),
-            'total_words': len(response_text.split()),
-            'temperature': temperature,
-            'max_length': max_length,
-            'model_used': False
+            "max_length": max_length,
+            "num_return_sequences": num_return_sequences,
+            "temperature": temperature,
+            "top_k": top_k,
+            "top_p": top_p,
+            "repetition_penalty": repetition_penalty,
+            "do_sample": True,
+            "early_stopping": early_stopping,
+            "pad_token_id": _tokenizer.eos_token_id,
         }
-    }
 
-@app.get("/")
-@app.head("/")
-async def root():
-    """
-    Root endpoint - health check
-    Supports both GET and HEAD requests
-    """
-    try:
-        model_status = "loaded" if get_model() else "not loaded"
-        response = {
-            "status": "ok",
-            "message": "ArpoChat API is running",
-            "model_status": model_status
-        }
-        return JSONResponse(content=response)
-    except Exception as e:
-        logger.error(f"Error in root endpoint: {str(e)}")
-        raise HTTPException(status_code=500, detail="Internal server error")
-
-@app.post("/generate")
-async def generate(request: GenerateRequest):
-    """
-    Text generation endpoint with fallback
-    """
-    try:
-        logger.info(f"Received generation request with prompt: {request.prompt[:50]}...")
+        # Generate sequences
+        with torch.no_grad():  # Disable gradient calculation
+            output_sequences = _model.generate(inputs, **gen_kwargs)
         
-        # Validate input
-        if not request.prompt.strip():
-            return JSONResponse(
-                status_code=400,
-                content={
-                    "success": False,
-                    "error": "Empty prompt provided"
-                }
-            )
+        # Decode and clean up the generated sequences
+        generated_sequences = []
+        for sequence in output_sequences:
+            text = _tokenizer.decode(sequence, skip_special_tokens=True)
+            # Remove the input prompt from the output
+            text = text[len(_tokenizer.decode(inputs[0], skip_special_tokens=True)):]
+            generated_sequences.append(text.strip())
         
-        # Try to ensure model is loaded
-        model = get_model()
-        if not model:
-            logger.error("Model failed to load")
-            return JSONResponse(
-                status_code=503,
-                content={
-                    "success": False,
-                    "error": "Model not available"
-                }
-            )
+        # Clear memory
+        del inputs, output_sequences
+        if torch.cuda.is_available():
+            torch.cuda.empty_cache()
+        gc.collect()
         
-        try:
-            # Generate responses for each temperature
-            responses = []
-            total_chars = 0
-            total_words = 0
-            total_tokens = 0
-            
-            for temp in request.temperatures:
-                try:
-                    generated_text = generate_text(
-                        model=model,
-                        prompt=request.prompt,
-                        max_length=request.max_length,
-                        temperature=temp
-                    )
-                    
-                    # Calculate stats
-                    chars = len(generated_text)
-                    words = len(generated_text.split())
-                    tokens = len(_tokenizer.encode(generated_text))
-                    
-                    total_chars += chars
-                    total_words += words
-                    total_tokens += tokens
-                    
-                    responses.append({
-                        'temperature': temp,
-                        'text': generated_text,
-                        'stats': {
-                            'chars': chars,
-                            'words': words,
-                            'tokens': tokens
-                        }
-                    })
-                except Exception as gen_error:
-                    logger.error(f"Error generating text for temperature {temp}: {str(gen_error)}")
-                    responses.append({
-                        'temperature': temp,
-                        'text': f"Error generating text: {str(gen_error)}",
-                        'error': True
-                    })
-            
-            # If we have at least one successful generation
-            if any(not response.get('error', False) for response in responses):
-                return GenerateResponse(
-                    success=True,
-                    prompt=request.prompt,
-                    responses=responses,
-                    stats={
-                        'total_chars': total_chars,
-                        'total_words': total_words,
-                        'total_tokens': total_tokens,
-                        'model_used': True
-                    }
-                )
-            else:
-                # If all generations failed, return fallback
-                return generate_fallback_response(
-                    prompt=request.prompt,
-                    max_length=request.max_length,
-                    temperature=request.temperatures[0] if request.temperatures else 0.7
-                )
-                
-        except Exception as gen_error:
-            logger.error(f"Error during generation process: {str(gen_error)}")
-            return JSONResponse(
-                status_code=500,
-                content={
-                    "success": False,
-                    "error": str(gen_error),
-                    "message": "Error during text generation"
-                }
-            )
-            
+        return generated_sequences
     except Exception as e:
-        logger.error(f"Error in generate endpoint: {str(e)}")
-        return JSONResponse(
-            status_code=500,
-            content={
-                "success": False,
-                "error": str(e),
-                "message": "Lo siento, hubo un error generando la respuesta"
-            }
-        ) 
\ No newline at end of file
+        logger.error(f"Error generating text: {str(e)}")
+        # Clear memory on error
+        if torch.cuda.is_available():
+            torch.cuda.empty_cache()
+        gc.collect()
+        raise 
\ No newline at end of file
diff --git a/api/main.py b/api/main.py
index 86f2630..0cbb80d 100644
--- a/api/main.py
+++ b/api/main.py
@@ -1,24 +1,27 @@
-import os
-import logging
-from fastapi import FastAPI, HTTPException
+from fastapi import FastAPI, HTTPException, Request
 from fastapi.middleware.cors import CORSMiddleware
 from fastapi.staticfiles import StaticFiles
-from fastapi.responses import FileResponse, JSONResponse, HTMLResponse
+from fastapi.responses import JSONResponse
 from pydantic import BaseModel
-from typing import Optional, List
-from api.generate import get_model, generate_text, _tokenizer
-from transformers import AutoTokenizer
+from typing import List, Optional
+import logging
+from api.generate import generate_text, get_model
+import os
 
 # Configure logging
 logging.basicConfig(
-    level=logging.DEBUG,
+    level=logging.INFO,
     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
 )
 logger = logging.getLogger(__name__)
 
-app = FastAPI()
+app = FastAPI(
+    title="ArpoChat API",
+    description="API for ArpoChat text generation",
+    version="1.0.0"
+)
 
-# Configure CORS
+# Enable CORS
 app.add_middleware(
     CORSMiddleware,
     allow_origins=["*"],
@@ -27,97 +30,64 @@ app.add_middleware(
     allow_headers=["*"],
 )
 
-# Get the absolute path to the public directory
-PUBLIC_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "public")
-STATIC_DIR = os.path.join(PUBLIC_DIR, "static")
-
-# Mount static files directory
-app.mount("/static", StaticFiles(directory=STATIC_DIR, html=True), name="static")
+# Mount static files
+app.mount("/static", StaticFiles(directory="public/static"), name="static")
 
 class GenerateRequest(BaseModel):
     prompt: str
-    max_length: Optional[int] = 1000
-    temperatures: Optional[List[float]] = [0.2, 0.5, 0.8]
+    max_length: Optional[int] = 100
+    temperature: Optional[float] = 0.8
 
-@app.get("/")
-async def root():
-    """Serve the main HTML page"""
-    index_path = os.path.join(PUBLIC_DIR, "index.html")
-    if not os.path.exists(index_path):
-        raise HTTPException(status_code=404, detail="Index file not found")
-    return FileResponse(index_path)
+@app.get("/health")
+async def health_check():
+    """Health check endpoint to verify model status."""
+    try:
+        model = get_model()
+        if model is None:
+            return JSONResponse(
+                status_code=503,
+                content={"status": "error", "message": "Model not loaded"}
+            )
+        return JSONResponse(
+            content={"status": "ok", "message": "Model loaded and ready"}
+        )
+    except Exception as e:
+        logger.error(f"Health check failed: {str(e)}")
+        return JSONResponse(
+            status_code=503,
+            content={"status": "error", "message": str(e)}
+        )
 
 @app.post("/generate")
 async def generate(request: GenerateRequest):
-    """Generate text based on the prompt with multiple temperatures, finish all phrases and words and try to connect phrases evolving the text in big archs of expression, either dadaistic, oneiric, surrealist, or concrete using rhime"""
+    """Generate text from a prompt."""
     try:
         logger.info(f"Received generation request with prompt: {request.prompt[:50]}...")
         
-        # Get the model (this will load it if not already loaded)
-        model = get_model()
-        if not model:
-            logger.error("Failed to load model")
-            raise HTTPException(status_code=500, detail="Failed to load model")
-        
-        # Generate text for each temperature
-        responses = []
-        total_chars = 0
-        total_words = 0
-        total_tokens = 0
+        # Validate input
+        if not request.prompt.strip():
+            raise HTTPException(status_code=400, detail="Prompt cannot be empty")
         
-        for temp in request.temperatures:
-            generated_text = generate_text(
-                model=model,
-                prompt=request.prompt,
-                max_length=request.max_length,
-                temperature=temp
-            )
-            
-            # Calculate stats
-            chars = len(generated_text)
-            words = len(generated_text.split())
-            tokens = len(_tokenizer.encode(generated_text)) if _tokenizer else 0
-            
-            responses.append({
-                "temperature": temp,
-                "text": generated_text,
-                "stats": {
-                    "chars": chars,
-                    "words": words,
-                    "tokens": tokens
-                }
-            })
-            
-            total_chars += chars
-            total_words += words
-            total_tokens += tokens
+        # Generate text
+        responses = generate_text(
+            prompt=request.prompt,
+            max_length=request.max_length,
+            temperature=request.temperature
+        )
         
-        logger.info("Text generation successful")
-        return JSONResponse(content={
-            "success": True,
-            "prompt": request.prompt,
-            "responses": responses,
-            "stats": {
-                "total_chars": total_chars,
-                "total_words": total_words,
-                "total_tokens": total_tokens,
-                "avg_words_per_response": total_words / len(request.temperatures),
-                "avg_tokens_per_response": total_tokens / len(request.temperatures)
+        return JSONResponse(
+            content={
+                "status": "success",
+                "responses": responses,
+                "prompt": request.prompt
             }
-        })
-        
+        )
     except Exception as e:
-        logger.error(f"Error in /generate endpoint: {str(e)}")
+        logger.error(f"Error generating text: {str(e)}")
         return JSONResponse(
             status_code=500,
             content={
-                "success": False,
-                "error": str(e),
-                "message": "Lo siento, hubo un error generando la respuesta"
+                "status": "error",
+                "message": f"Error generating text: {str(e)}"
             }
-        )
-
-@app.get("/health")
-async def health():
-    """Health check endpoint"""
-    return {"status": "ok"} 
\ No newline at end of file
+        ) 
\ No newline at end of file
diff --git a/generate.py b/generate.py
deleted file mode 100644
index b376c37..0000000
--- a/generate.py
+++ /dev/null
@@ -1,224 +0,0 @@
-from fastapi import FastAPI, HTTPException, Request
-from fastapi.middleware.cors import CORSMiddleware
-from fastapi.responses import JSONResponse
-from pydantic import BaseModel
-import os
-from typing import Optional
-import logging
-import torch
-from transformers import GPT2LMHeadModel, GPT2Tokenizer
-
-# Configure logging
-logging.basicConfig(
-    level=logging.DEBUG,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
-)
-logger = logging.getLogger(__name__)
-
-app = FastAPI(
-    title="ArpoChat API",
-    description="API for ArpoChat text generation",
-    version="1.0.0"
-)
-
-# Enable CORS
-app.add_middleware(
-    CORSMiddleware,
-    allow_origins=["*"],
-    allow_credentials=True,
-    allow_methods=["*"],
-    allow_headers=["*"],
-)
-
-# Constants
-MODEL_ID = "animaratio/arpochat"
-DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
-
-# Global variables for model caching
-_model = None
-_tokenizer = None
-
-def get_model():
-    """Load and cache the model and tokenizer."""
-    global _model, _tokenizer
-    
-    try:
-        if _model is None:
-            logger.info(f"Loading model from {MODEL_ID}")
-            
-            # Get Hugging Face token from environment
-            hf_token = os.getenv('HF_TOKEN')
-            if not hf_token:
-                raise ValueError("HF_TOKEN environment variable not set")
-            
-            # Load tokenizer and model
-            _tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
-            _model = GPT2LMHeadModel.from_pretrained(
-                MODEL_ID,
-                token=hf_token,
-                trust_remote_code=True
-            )
-            
-            # Configure model
-            _tokenizer.pad_token = _tokenizer.eos_token
-            _model.config.pad_token_id = _model.config.eos_token_id
-            _model.to(DEVICE)
-            _model.eval()
-            
-            logger.info("Model loaded successfully")
-        
-        return _model
-    except Exception as e:
-        logger.error(f"Error loading model: {str(e)}")
-        return None
-
-def generate_text(
-    model: GPT2LMHeadModel,
-    prompt: str,
-    max_length: Optional[int] = 100,
-    temperature: Optional[float] = 0.7
-) -> str:
-    """Generate text using the loaded model."""
-    try:
-        # Encode the prompt
-        inputs = _tokenizer(prompt, return_tensors='pt', padding=True)
-        input_ids = inputs['input_ids'].to(DEVICE)
-        attention_mask = inputs['attention_mask'].to(DEVICE)
-        
-        # Set generation parameters
-        gen_kwargs = {
-            'input_ids': input_ids,
-            'attention_mask': attention_mask,
-            'max_length': max_length,
-            'num_return_sequences': 1,
-            'do_sample': True,
-            'temperature': temperature,
-            'top_k': 50,
-            'top_p': 0.95,
-            'no_repeat_ngram_size': 3,
-            'num_beams': 5,
-            'pad_token_id': _tokenizer.eos_token_id,
-            'early_stopping': True
-        }
-        
-        # Generate text
-        with torch.no_grad():
-            output_sequences = model.generate(**gen_kwargs)
-        
-        # Decode and clean up the response
-        generated_text = _tokenizer.decode(output_sequences[0], skip_special_tokens=True)
-        generated_text = generated_text.replace(prompt, '').strip()
-        
-        return generated_text
-        
-    except Exception as e:
-        logger.error(f"Error generating text: {str(e)}")
-        raise
-
-def generate_text_with_model(prompt: str, max_length: int, temperature: float) -> dict:
-    """Generate text using the ML model"""
-    try:
-        outputs = _model(prompt,
-                       max_length=max_length,
-                       temperature=temperature,
-                       num_return_sequences=1,
-                       do_sample=True)
-        
-        generated_text = outputs[0]['generated_text']
-        
-        # Calculate stats
-        stats = {
-            'total_chars': len(generated_text),
-            'total_words': len(generated_text.split()),
-            'total_tokens': len(_tokenizer.encode(generated_text)),
-            'temperature': temperature,
-            'max_length': max_length,
-            'model_used': True
-        }
-        
-        return {
-            'generated_text': generated_text,
-            'prompt': prompt,
-            'stats': stats
-        }
-    except Exception as e:
-        logger.error(f"Error generating text: {str(e)}")
-        raise Exception(f"Error generating text: {str(e)}")
-
-def generate_fallback_response(prompt: str, max_length: int, temperature: float) -> dict:
-    """Generate a fallback response when the model is not available"""
-    response_text = f"Test response for prompt: {prompt}"
-    return {
-        'generated_text': response_text,
-        'prompt': prompt,
-        'stats': {
-            'total_chars': len(response_text),
-            'total_words': len(response_text.split()),
-            'temperature': temperature,
-            'max_length': max_length,
-            'model_used': False
-        }
-    }
-
-@app.get("/")
-@app.head("/")
-async def root():
-    """
-    Root endpoint - health check
-    Supports both GET and HEAD requests
-    """
-    try:
-        model_status = "loaded" if get_model() else "not loaded"
-        response = {
-            "status": "ok",
-            "message": "ArpoChat API is running",
-            "model_status": model_status
-        }
-        return JSONResponse(content=response)
-    except Exception as e:
-        logger.error(f"Error in root endpoint: {str(e)}")
-        raise HTTPException(status_code=500, detail="Internal server error")
-
-@app.post("/api/generate")
-async def generate(request: GenerateRequest):
-    """
-    Text generation endpoint with fallback
-    """
-    try:
-        logger.info(f"Received generation request with prompt: {request.prompt[:50]}...")
-        
-        # Try to ensure model is loaded
-        model_available = get_model()
-        
-        if model_available:
-            try:
-                # Try to generate with model
-                result = generate_text_with_model(
-                    prompt=request.prompt,
-                    max_length=request.max_length,
-                    temperature=request.temperature
-                )
-                logger.info("Successfully generated text with model")
-                return GenerateResponse(**result)
-            except Exception as model_error:
-                logger.warning(f"Model generation failed: {str(model_error)}")
-                # Fall back to test response if model fails
-                result = generate_fallback_response(
-                    prompt=request.prompt,
-                    max_length=request.max_length,
-                    temperature=request.temperature
-                )
-                return GenerateResponse(**result)
-        else:
-            # Use fallback if model is not available
-            logger.warning("Model not available, using fallback response")
-            result = generate_fallback_response(
-                prompt=request.prompt,
-                max_length=request.max_length,
-                temperature=request.temperature
-            )
-            return GenerateResponse(**result)
-            
-    except Exception as e:
-        logger.error(f"Error processing request: {str(e)}")
-        raise HTTPException(status_code=500, detail=str(e)) 
\ No newline at end of file
diff --git a/main.py b/main.py
deleted file mode 100644
index 3ab7d8b..0000000
--- a/main.py
+++ /dev/null
@@ -1,108 +0,0 @@
-import os
-import logging
-from fastapi import FastAPI, HTTPException
-from fastapi.middleware.cors import CORSMiddleware
-from fastapi.staticfiles import StaticFiles
-from fastapi.responses import FileResponse, JSONResponse, HTMLResponse
-from pydantic import BaseModel
-from typing import Optional, List
-from api.generate import get_model, generate_text
-
-# Configure logging
-logging.basicConfig(
-    level=logging.DEBUG,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
-)
-logger = logging.getLogger(__name__)
-
-app = FastAPI()
-
-# Configure CORS
-app.add_middleware(
-    CORSMiddleware,
-    allow_origins=["*"],
-    allow_credentials=True,
-    allow_methods=["*"],
-    allow_headers=["*"],
-)
-
-# Get the absolute path to the public directory
-PUBLIC_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "public")
-STATIC_DIR = os.path.join(PUBLIC_DIR, "static")
-
-# Mount static files directory
-app.mount("/static", StaticFiles(directory=STATIC_DIR, html=True), name="static")
-
-class GenerateRequest(BaseModel):
-    prompt: str
-    max_length: Optional[int] = 100
-    temperatures: Optional[List[float]] = [0.2, 0.5, 0.8]
-
-@app.get("/")
-async def root():
-    """Serve the main HTML page"""
-    index_path = os.path.join(PUBLIC_DIR, "index.html")
-    if not os.path.exists(index_path):
-        raise HTTPException(status_code=404, detail="Index file not found")
-    return FileResponse(index_path)
-
-@app.post("/generate")
-async def generate(request: GenerateRequest):
-    """Generate text based on the prompt with multiple temperatures"""
-    try:
-        logger.info(f"Received generation request with prompt: {request.prompt[:50]}...")
-        
-        # Get the model (this will load it if not already loaded)
-        model = get_model()
-        if not model:
-            logger.error("Failed to load model")
-            raise HTTPException(status_code=500, detail="Failed to load model")
-        
-        # Generate text for each temperature
-        responses = []
-        total_chars = 0
-        total_words = 0
-        
-        for temp in request.temperatures:
-            generated_text = generate_text(
-                model=model,
-                prompt=request.prompt,
-                max_length=request.max_length,
-                temperature=temp
-            )
-            
-            responses.append({
-                "temperature": temp,
-                "text": generated_text
-            })
-            
-            total_chars += len(generated_text)
-            total_words += len(generated_text.split())
-        
-        logger.info("Text generation successful")
-        return JSONResponse(content={
-            "success": True,
-            "prompt": request.prompt,
-            "responses": responses,
-            "stats": {
-                "total_chars": total_chars,
-                "total_words": total_words,
-                "avg_words_per_response": total_words / len(request.temperatures)
-            }
-        })
-        
-    except Exception as e:
-        logger.error(f"Error in /generate endpoint: {str(e)}")
-        return JSONResponse(
-            status_code=500,
-            content={
-                "success": False,
-                "error": str(e),
-                "message": "Lo siento, hubo un error generando la respuesta"
-            }
-        )
-
-@app.get("/health")
-async def health():
-    """Health check endpoint"""
-    return {"status": "ok"} 
\ No newline at end of file
diff --git a/render.yaml b/render.yaml
index f996945..5347299 100644
--- a/render.yaml
+++ b/render.yaml
@@ -3,17 +3,23 @@ services:
     name: arpochat
     env: python
     buildCommand: pip install -r requirements.txt
-    startCommand: uvicorn api.main:app --host 0.0.0.0 --port $PORT --workers 1 --log-level info
+    startCommand: uvicorn api.main:app --host 0.0.0.0 --port $PORT
     envVars:
       - key: PYTHON_VERSION
-        value: 3.10.13
-      - key: TRANSFORMERS_CACHE
+        value: 3.11
+      - key: HF_TOKEN
+        sync: false
+      - key: HF_HOME
         value: /tmp/.cache/huggingface
+      - key: HF_HUB_CACHE
+        value: /tmp/.cache/huggingface/hub
       - key: TORCH_HOME
         value: /tmp/.cache/torch
-      - key: LOG_LEVEL
-        value: info
+    plan: free
+    disk:
+      name: cache
+      mountPath: /tmp/.cache
+      sizeGB: 1
     healthCheckPath: /health
     autoDeploy: true
-    numInstances: 1
-    plan: free 
\ No newline at end of file
+    numInstances: 1 
\ No newline at end of file
